<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
  <!-- Replace the content tag with appropriate information -->
  <meta name="description" content="PointingImgEst">
  <meta property="og:title" content="PointingImgEst"/>
  <meta property="og:description" content="SOCIAL MEDIA DESCRIPTION TAG TAG"/>
  <meta property="og:url" content="https://emergentsystemlabstudent.github.io/PointingImgEst/"/>
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X630-->
  <meta property="og:image" content="static/image/your_banner_image.png" />
  <meta property="og:image:width" content="1200"/>
  <meta property="og:image:height" content="630"/>


  <meta name="twitter:title" content="TWITTER BANNER TITLE META TAG">
  <meta name="twitter:description" content="TWITTER BANNER DESCRIPTION META TAG">
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X600-->
  <meta name="twitter:image" content="static/images/your_twitter_banner_image.png">
  <meta name="twitter:card" content="summary_large_image">
  <!-- Keywords for your paper to be indexed by-->
  <meta name="keywords" content="KEYWORDS SHOULD BE PLACED HERE">
  <meta name="viewport" content="width=device-width, initial-scale=1">


  <title>PointingImgEst</title>
  <link rel="icon" type="image/x-icon" href="static/images/favicon.ico">
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
  rel="stylesheet">

  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>
  <script src="carousel.js"></script>
</head>

<body>


<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">Pointing Frame Estimation with Audio-Visual Time Series Data for Daily Life Service Robots</h1>
          <div class="is-size-5 publication-authors">
            <!-- Paper authors -->
            <span class="author-block">
              Hikaru Nakagawa<sup>1</sup>,</span>
            <span class="author-block">
              <a href="https://scholar.google.co.jp/citations?user=KPxSCJUAAAAJ&hl=ja&oi=ao" target="_blank">Shoichi Hasegawa<sup>1,*</sup>,</span>
            <span class="author-block">
              <a href="https://scholar.google.co.jp/citations?user=Y4qjYvMAAAAJ&hl=ja&oi=ao" target="_blank">Yoshinobu Hagiwara<sup>1,2</sup>,</a></span>
            <span class="author-block">
              <a href="https://scholar.google.co.jp/citations?user=jtB7J0AAAAAJ&hl=ja&oi=ao" target="_blank">Akira Taniguchi<sup>1</sup>,</a></span>
            <span class="author-block">
              <a href="https://scholar.google.co.jp/citations?user=dPOCLQEAAAAJ&hl=ja&oi=ao" target="_blank">Tadahiro Taniguchi<sup>1,3</sup></a></span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <sup>1</sup>Ritsumeikan University,
              <sup>2</sup>Soka University,
              <sup>3</sup>Kyoto University
              <br><span class="publication-awards">Winner of the Best Paper Award in IEEE SMC 2024 (1st of 755)!</span>
            </span>
            <span class="eql-cntrb"><small><br><sup>*</sup>Corresponding Author</small></span>
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- Arxiv PDF link -->
              <span class="link-block">
                <a href="" target="_blank"
                class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>
                    Paper (coming soon)
                  </span>
                </a>
              </span>

              <!-- Slide link -->
              <span class="link-block">
                <a href="https://speakerdeck.com/shoichi_hasegawa/pointing-frame-estimation-with-audio-visual-time-series-data-for-daily-life-service-robots" target="_blank"
                class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Slide</span>
                </a>
              </span>

              <!-- Github link -->
              <span class="link-block">
                <a href="https://github.com/EmergentSystemLabStudent/PointingImgEst/tree/devel" target="_blank"
                class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                </a>
              </span>

              <!-- ArXiv abstract Link -->
              <!-- <span class="link-block">
                <a href="https://arxiv.org/abs/<ARXIV PAPER ID>" target="_blank"
                class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span> -->

            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>


<!-- Teaser video-->
<!-- <section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <video poster="" id="tree" autoplay controls muted loop height="100%">
        <source src="static/videos/banner_video.mp4"
        type="video/mp4">
      </video>
      <h2 class="subtitle has-text-centered">
        Aliquam vitae elit ullamcorper tellus egestas pellentesque. Ut lacus tellus, maximus vel lectus at, placerat pretium mi. Maecenas dignissim tincidunt vestibulum. Sed consequat hendrerit nisl ut maximus.
      </h2>
    </div>
  </div>
</section> -->
<!-- End teaser video -->

<!-- Paper abstract -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Daily life support robots in the home environment interpret the user's pointing and understand the instructions, thereby increasing the number of instructions accomplished.
            This study aims to improve the estimation performance of pointing frames by using speech information when a person gives pointing or verbal instructions to the robot. 
            The estimation of the pointing frame, which represents the moment when the user points, can help the user understand the instructions.
            Therefore, we perform pointing frame estimation using a time-series model, utilizing the user's speech, images, and speech-recognized text observed by the robot.
            In our experiments, we set up realistic communication conditions, such as speech containing everyday conversation, non-upright posture, actions other than pointing, and reference objects outside the robot's field of view. 
            The results showed that adding speech information improved the estimation performance, especially the Transformer model with Mel-Spectrogram as a feature. 
            This study will lead to be applied to object localization and action planning in 3D environments by robots in the future.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End paper abstract -->

<!-- Image carousel -->
<section class="hero is-small">
  <div class="hero-body">
    <div class="container">
      <h2 class="title is-3">Overview</h2>
      <!-- <div id="results-carousel" class="carousel results-carousel"> -->
        <div class="item">
          <img src="static/images/overview.svg" alt="image of model"/>
          <h2 class="subtitle has-text-centered">
            An overview of our study. The robot makes an estimation of the moment at which a person gives a pointing instruction. 
            The robot uses the speech uttered by the person, the video showing the pointing, and the sentence for the estimation.
          </h2>
        </div>
        <!-- <div class="item">
          <img src="static/images/carousel2.jpg" alt="MY ALT TEXT"/>
          <h2 class="subtitle has-text-centered">
            Second image description.
          </h2>
        </div>
        <div class="item">
          <img src="static/images/carousel3.jpg" alt="MY ALT TEXT"/>
          <h2 class="subtitle has-text-centered">
            Third image description.
          </h2>
        </div>
        <div class="item">
          <img src="static/images/carousel4.jpg" alt="MY ALT TEXT"/>
          <h2 class="subtitle has-text-centered">
            Fourth image description.
          </h2>
        </div> -->
      <!-- </div> -->
    </div>
  </div>
</section>
<!-- End image carousel -->

<!-- Image carousel -->
<section class="hero is-small">
  <div class="hero-body">
    <div class="container">
      <h2 class="title is-3">Pointing Frame Estimator with Audio-Visual Time Series Data</h2>
      <!-- <div id="results-carousel" class="carousel results-carousel"> -->
        <div class="item">
          <img src="static/images/model.svg" alt="image of model"/>
          <h2 class="subtitle has-text-centered">
            An overview of our study. The robot makes an estimation of the moment at which a person gives a pointing instruction. 
            The robot uses the speech uttered by the person, the video showing the pointing, and the sentence for the estimation.
          </h2>
        </div>
        <!-- <div class="item">
          <img src="static/images/carousel2.jpg" alt="MY ALT TEXT"/>
          <h2 class="subtitle has-text-centered">
            Second image description.
          </h2>
        </div>
        <div class="item">
          <img src="static/images/carousel3.jpg" alt="MY ALT TEXT"/>
          <h2 class="subtitle has-text-centered">
            Third image description.
          </h2>
        </div>
        <div class="item">
          <img src="static/images/carousel4.jpg" alt="MY ALT TEXT"/>
          <h2 class="subtitle has-text-centered">
            Fourth image description.
          </h2>
        </div> -->
      <!-- </div> -->
    </div>
  </div>
</section>
<!-- End image carousel -->


<!-- video -->
<section class="hero is-small is-light">
  <div class="hero-body">
    <div class="container">
      <h2 class="title is-3">Scenarios of Instructions</h2>
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <!-- Video Carousel -->
          <div class="results-carousel">
            <!-- First Video -->
            <div class="video-section active">
              <h3 class="title is-3">A scenario involving verbal utterances including daily conversations</h3>
              <video controls style="width: 700px; height: auto;" preload="metadata">
                <source src="static/videos/other_utterance_except_instruction.mp4" type="video/mp4">
              </video>
            </div>

            <!-- Second Video -->
            <div class="video-section">
              <h3 class="title is-3">A scenario in which a user's posture is not consistently upright</h3>
              <video controls style="width: 700px; height: auto;" preload="metadata">
                <source src="static/videos/no_upright.mp4" type="video/mp4">
              </video>
            </div>

            <!-- Third Video -->
            <div class="video-section">
              <h3 class="title is-3">A scenario in which a user's movements include actions beyond pointing</h3>
              <video controls style="width: 700px; height: auto;" preload="metadata">
                <source src="static/videos/other_action_except_pointing.mp4" type="video/mp4">
              </video>
            </div>

            <!-- Fourth Video -->
            <div class="video-section">
              <h3 class="title is-3">A scenario in which the referenced object is outside the visual scope of the robot</h3>
              <video controls style="width: 700px; height: auto;" preload="metadata">
                <source src="static/videos/no_reference_object.mp4" type="video/mp4">
              </video>
            </div>
          </div>
          <!-- Navigation Buttons -->
          <div class="carousel-controls">
            <button id="prev-button" class="button is-dark">Previous</button>
            <button id="next-button" class="button is-dark">Next</button>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>


<!-- End video -->


<!-- Video carousel -->
<!-- <section class="hero is-small">
  <div class="hero-body">
    <div class="container">
      <h2 class="title is-3">Global localization</h2>
      <div id="results-carousel" class="carousel results-carousel">
        <div class="item item-video1">
          <video poster="" id="video1" autoplay controls muted loop height="100%">
            <source src="static/videos/carousel1.mp4"
            type="video/mp4">
          </video>
        </div>
        <div class="item item-video2">
          <video poster="" id="video2" autoplay controls muted loop height="100%">
            <source src="static/videos/carousel2.mp4"
            type="video/mp4">
          </video>
        </div>
        <div class="item item-video3">
          <video poster="" id="video3" autoplay controls muted loop height="100%">\
            <source src="static/videos/carousel3.mp4"
            type="video/mp4">
          </video>
        </div>
        <div class="item item-video4">
          <video poster="" id="video4" autoplay controls muted loop height="100%">\
            <source src="static/videos/carousel4.mp4"
            type="video/mp4">
          </video>
        </div>
      </div>
    </div>
  </div>
</section> -->
<!-- End video carousel -->

<!-- Image carousel -->
<section class="hero is-small">
  <div class="hero-body">
    <div class="container">
      <h2 class="title is-3">Learned representation(Environment2)</h2>
      <p>
        The representation obtained in the latent variable t_t (200 dimensions).<br>
        Model 2 on the left shows more sparse features than Model 1.
      </p>
      <!-- <div id="results-carousel" class="carousel results-carousel"> -->

        <div class="item">
          <img src="static/images/representations.jpg" alt="image of representations"/>
          <!-- <iframe  src="static/pdfs/model2.pdf" width="100%" height="550"></iframe> -->
        </div>

      <!-- </div> -->
    </div>
  </div>
</section>
<!-- End image carousel -->


<!-- Paper poster -->
<!-- <section class="hero is-small is-light">
  <div class="hero-body">
    <div class="container">
      <h2 class="title">Poster</h2>

      <iframe  src="static/pdfs/sample.pdf" width="100%" height="550">
        </iframe>

    </div>
  </div>
</section> -->
<!--End paper poster -->


<!--BibTex citation -->
<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <!-- <pre><code>BibTex Code Here</code></pre> -->
    <pre><code>
      @inproceedings{nakagawa2024pointing,
        author={Nakagawa, Hikaru  and Hasegawa, Shoichi  and Hagiwara, Yoshinobu  and Taniguchi, Akira  and Taniguchi, Tadahiro},
        title={Pointing Frame Estimation with Audio-Visual Time Series Data for Daily Life Service Robots},
        booktitle={IEEE International Conference on Systems, Man, and Cybernetics (SMC)},
        year={2024, in press}
      }
    </code></pre>
  </div>
</section>
<!--End BibTex citation -->

<!--Acknowledgements citation -->
<section class="section" id="Acknowledgements">
  <div class="container is-max-desktop content">
    <!-- <h2 class="title">Acknowledgements</h2> -->
    <h2 class="title">Funding</h2>
    <p>
      This work was supported by JSPS KAKENHI Grants-in-Aid for Scientific Research (Grant Numbers JP23K16975, 22K12212), JST Moonshot Research & Development Program (Grant Number JPMJMS2011), and JST SPRING, Grant Number JPMJSP2101.
    </p>
  </div>
</section>
<!--End Acknowledgements citation -->


<footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">

          <p>
            This page was built using the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template" target="_blank">Academic Project Page Template</a> which was adopted from the <a href="https://nerfies.github.io" target="_blank">Nerfies</a> project page.
            You are free to borrow the of this website, we just ask that you link back to this page in the footer. <br> This website is licensed under a <a rel="license"  href="http://creativecommons.org/licenses/by-sa/4.0/" target="_blank">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>

        </div>
      </div>
    </div>
  </div>
</footer>

<!-- Statcounter tracking code -->

<!-- You can add a tracker to track page visits by creating an account at statcounter.com -->

<!-- End of Statcounter Code -->

  </body>
  </html>
